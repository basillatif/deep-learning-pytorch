{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IS675 Lab 10: Deep Learning For Sequential Data\n",
        "\n",
        "## Future total sales of pizza\n",
        "In this lab, we will predict the future total sales of pizza based on the sequential transactions by using LSTM.\n",
        "\n",
        "\n",
        "This pizza sales dataset make up 12 relevant features:\n",
        "\n",
        "**order_id:** Unique identifier for each order placed by a table\n",
        "\n",
        "**order_details_id:** Unique identifier for each pizza placed within each order (pizzas of the same type and size are kept in the same row, and the quantity increases)\n",
        "\n",
        "\n",
        "**pizza_id:** Unique key identifier that ties the pizza ordered to its details, like size and price\n",
        "\n",
        "\n",
        "**quantity:** Quantity ordered for each pizza of the same type and size\n",
        "\n",
        "**order_date:** Date the order was placed (entered into the system prior to cooking & serving)\n",
        "\n",
        "**order_time:** Time the order was placed (entered into the system prior to cooking & serving)\n",
        "\n",
        "**unit_price:** Price of the pizza in USD\n",
        "\n",
        "**total_price:** unit_price * quantity\n",
        "\n",
        "**pizza_size:** Size of the pizza (Small, Medium, Large, X Large, or XX Large)\n",
        "\n",
        "**pizza_type:** Unique key identifier that ties the pizza ordered to its details, like size and price\n",
        "\n",
        "**pizza_ingredients:** ingredients used in the pizza as shown in the menu (they all include Mozzarella Cheese, even if not specified; and they all include Tomato Sauce, unless another sauce is specified)\n",
        "\n",
        "**pizza_name:** Name of the pizza as shown in the menu"
      ],
      "metadata": {
        "id": "u554wlVnjgjD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEQfZ8KsjYzd",
        "outputId": "6e23a549-4df2-4bc2-d986-14cdc233af0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from itertools import chain\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "UHZhPvpikgaB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Upload and clean data"
      ],
      "metadata": {
        "id": "ie6dzqI2kq9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data\n",
        "PizzaSales = pd.read_csv(\"/content/drive/MyDrive/IS675_data/PizzaSales.csv\")\n",
        "PizzaSales"
      ],
      "metadata": {
        "id": "0VkFP8HGkw0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine variable type\n",
        "PizzaSales.dtypes"
      ],
      "metadata": {
        "id": "dFlNEddXldw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine data size\n",
        "PizzaSales.shape"
      ],
      "metadata": {
        "id": "rIs_kZQDlgiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Simple data exploration"
      ],
      "metadata": {
        "id": "CQQlVx1hlkBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert order_date column to datetime\n",
        "PizzaSales['order_date'] = pd.to_datetime(PizzaSales['order_date'])"
      ],
      "metadata": {
        "id": "CGaHnwLplox2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the number of pizza sold by month\n",
        "PizzaSales['order_month'] =pd.DatetimeIndex(PizzaSales['order_date']).month\n",
        "PizzaSales.loc[(PizzaSales['order_month'] ==1), 'order_month'] = 'January'\n",
        "PizzaSales.loc[(PizzaSales['order_month'] ==2), 'order_month'] = 'February'\n",
        "PizzaSales.loc[(PizzaSales['order_month'] ==3), 'order_month'] = 'March'\n",
        "PizzaSales.loc[(PizzaSales['order_month'] ==4), 'order_month'] = 'April'\n",
        "PizzaSales.loc[(PizzaSales['order_month'] ==5), 'order_month'] = 'May'\n",
        "PizzaSales.loc[(PizzaSales['order_month'] ==6), 'order_month'] = 'June'\n",
        "PizzaSales.loc[(PizzaSales['order_month'] ==7), 'order_month'] = 'July'\n",
        "PizzaSales.loc[(PizzaSales['order_month'] ==8), 'order_month'] = 'August'\n",
        "PizzaSales.loc[(PizzaSales['order_month'] ==9), 'order_month'] = 'September'\n",
        "PizzaSales.loc[(PizzaSales['order_month'] ==10), 'order_month'] = 'October'\n",
        "PizzaSales.loc[(PizzaSales['order_month'] ==11), 'order_month'] = 'November'\n",
        "PizzaSales.loc[(PizzaSales['order_month'] ==12), 'order_month'] = 'December'\n",
        "PizzaSales['order_month'].value_counts()"
      ],
      "metadata": {
        "id": "BXNNWAERltgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the number of pizza sold per day\n",
        "PizzaSales[\"Dayofweek\"] = PizzaSales['order_date'].dt.day_name()\n",
        "PizzaSales[\"Dayofweek\"].value_counts()"
      ],
      "metadata": {
        "id": "sABs30Mnlw52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the number of pizza sold by hour\n",
        "PizzaSales[['Hour','Minute', 'Second']]= PizzaSales['order_time'].str.split(\":\",expand=True)\n",
        "PizzaSales[\"Hour\"].value_counts()"
      ],
      "metadata": {
        "id": "yc3DSNeol1Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the number of pizza sold by pizza id\n",
        "PizzaSales[\"pizza_id\"].value_counts()"
      ],
      "metadata": {
        "id": "_Dsdub2gl5ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the number of pizza sold by pizza size\n",
        "PizzaSales[\"pizza_size\"].value_counts()"
      ],
      "metadata": {
        "id": "aiu1CowFl8Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PizzaSales['order_id'].max()"
      ],
      "metadata": {
        "id": "kH7W-U-xl-zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the average order value\n",
        "PizzaSales['total_price'].agg('sum')/PizzaSales['order_id'].max()"
      ],
      "metadata": {
        "id": "4nuVC22zmE3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Partition the data set for pizza sales prediction"
      ],
      "metadata": {
        "id": "XW9X8hjRmKV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Organize sales by date\n",
        "sales_by_date = PizzaSales.groupby(['order_date']).sum()\n",
        "sales_by_date[\"Dayofweek\"] = sales_by_date.index.day_name()\n",
        "sales_by_date = sales_by_date[['quantity','total_price','Dayofweek']]\n",
        "sales_by_date"
      ],
      "metadata": {
        "id": "5Xn975nNmNIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dummy variables\n",
        "sales_by_date = pd.get_dummies(sales_by_date, columns=['Dayofweek'],drop_first=True)\n",
        "sales_by_date"
      ],
      "metadata": {
        "id": "Rev6ZaM_mSeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the total sales\n",
        "total_sales=sales_by_date.rename(columns={'total_price':'total_sales'}).iloc[3:len(sales_by_date),1].reset_index(drop=True)\n",
        "total_sales"
      ],
      "metadata": {
        "id": "QvkPQ6bAmWBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply standardization\n",
        "numeric_variables = sales_by_date[['quantity', 'total_price']]\n",
        "scaler_s = StandardScaler().fit(numeric_variables)\n",
        "standard_variables = scaler_s.transform(numeric_variables)\n",
        "sales_by_date[['quantity', 'total_price']] = standard_variables\n",
        "numeric_variables"
      ],
      "metadata": {
        "id": "hg9_cYjdmWyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sequences\n",
        "seq1=sales_by_date.iloc[0:len(sales_by_date)-3].reset_index(drop=True)\n",
        "seq2=sales_by_date.iloc[1:len(sales_by_date)-2].reset_index(drop=True)\n",
        "seq3=sales_by_date.iloc[2:len(sales_by_date)-1].reset_index(drop=True)\n",
        "seq_data=pd.concat([seq1,seq2,seq3,total_sales],axis=1)\n",
        "seq_data"
      ],
      "metadata": {
        "id": "x6Ucn8SFmbLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Partition the data\n",
        "target = seq_data['total_sales']\n",
        "predictors = seq_data.drop(['total_sales'], axis=1)\n",
        "predictors_train, predictors_test, target_train, target_test = train_test_split(predictors, target, test_size = 0.3, random_state = 0)\n",
        "print(predictors_train.shape, predictors_test.shape, target_train.shape, target_test.shape)"
      ],
      "metadata": {
        "id": "KPMq24vMmiLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the distribution of target variable for training data set\n",
        "snsplot = sns.histplot(data = target_train)\n",
        "snsplot.set_title(\"Histogram of total_sales in the training data set\")"
      ],
      "metadata": {
        "id": "dJfnPE67mkaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the distribution of target variable for testing data set\n",
        "snsplot = sns.histplot(data = target_test)\n",
        "snsplot.set_title(\"Histogram of total_sales in the testing data set\")"
      ],
      "metadata": {
        "id": "JbqCSwyJmna2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Neural network prediction and evaluation"
      ],
      "metadata": {
        "id": "NTX25dHRm0on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a neural network on training data\n",
        "class extract_tensor(nn.Module):\n",
        "    def forward(self,x):\n",
        "        tensor, _ = x\n",
        "        return tensor[:, -1, :]\n",
        "\n",
        "class neural_network(nn.Module):\n",
        "          def __init__(self, in_size,hidden_size1,hidden_size2,out_size):\n",
        "            super().__init__()\n",
        "            self.network=nn.Sequential(\n",
        "                nn.LSTM(in_size,hidden_size1,num_layers=2,batch_first=True),\n",
        "                extract_tensor(),\n",
        "                nn.Linear(hidden_size1,hidden_size2),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_size2,out_size))\n",
        "          def forward(self,x):\n",
        "              out=self.network(x)\n",
        "              return out.squeeze()"
      ],
      "metadata": {
        "id": "6Pg0VkA_m_Bz"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tensor from pandas dataframe\n",
        "predictors_train_tensor = torch.tensor(predictors_train.values).view(248, 3, 8) # reshape the tensor to 248 sequences; each sequence has length = 3 and input size = 8 (will be used to predict the next day's sale based on the sales and orders of the past three days)\n",
        "target_train_tensor = torch.tensor(target_train.values)\n",
        "predictors_test_tensor = torch.tensor(predictors_test.values).view(107, 3, 8)\n",
        "target_test_tensor = torch.tensor(target_test.values)\n",
        "\n",
        "# Create tensor dataset (set target variable to float type)\n",
        "train_dataset = torch.utils.data.TensorDataset(predictors_train_tensor.float(), target_train_tensor.float())\n",
        "test_dataset = torch.utils.data.TensorDataset(predictors_test_tensor.float(), target_test_tensor.float())\n",
        "\n",
        "# Define training and testing data loader, and set batch size to 16\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "A0JWniLunCFQ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training loop function\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "    for epoch in range(0, n_epochs):\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        loss_train = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_train += loss.item()\n",
        "\n",
        "        if epoch == 0 or epoch == n_epochs-1 or epoch % 50 == 0:\n",
        "            print('Epoch {}, Training loss {}'.format(epoch, loss_train / len(train_loader)))"
      ],
      "metadata": {
        "id": "K3N-UUmNokDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "torch.manual_seed(0)\n",
        "model=neural_network(8,64,64,1)\n",
        "optimizer=optim.Adam(model.parameters())\n",
        "loss_fn=nn.MSELoss()\n",
        "\n",
        "training_loop(n_epochs=200, optimizer=optimizer,model=model,loss_fn=loss_fn,train_loader=train_loader)"
      ],
      "metadata": {
        "id": "SeFzI0mxoO15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define testing function\n",
        "def test(model, train_loader, test_loader):\n",
        "\n",
        "  # testing phase\n",
        "  model.eval()\n",
        "  predict_train = []\n",
        "  predict_test = []\n",
        "  label_train = []\n",
        "  label_test = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for inputs, labels in train_loader:\n",
        "          outputs = model(inputs)\n",
        "          predict_train.append(outputs.tolist())\n",
        "          label_train.append(labels.tolist())\n",
        "\n",
        "      for inputs, labels in test_loader:\n",
        "          outputs = model(inputs)\n",
        "          predict_test.append(outputs.tolist())\n",
        "          label_test.append(labels.tolist())\n",
        "\n",
        "  MAE_train = mean_absolute_error(list(chain(*label_train)), list(chain(*predict_train)))\n",
        "  RMSE_train = mean_squared_error(list(chain(*label_train)), list(chain(*predict_train)), squared=False)\n",
        "\n",
        "  MAE_test = mean_absolute_error(list(chain(*label_test)), list(chain(*predict_test)))\n",
        "  RMSE_test = mean_squared_error(list(chain(*label_test)), list(chain(*predict_test)), squared=False)\n",
        "\n",
        "  print(\"Training MAE and RMSE:\", MAE_train, RMSE_train)\n",
        "  print()\n",
        "  print(\"testing MAE and RMSE:\", MAE_test, RMSE_test)"
      ],
      "metadata": {
        "id": "f0O2ZlAWotlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine evaluation results\n",
        "test(model, train_loader, test_loader)"
      ],
      "metadata": {
        "id": "A6gkjL7loxGu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}